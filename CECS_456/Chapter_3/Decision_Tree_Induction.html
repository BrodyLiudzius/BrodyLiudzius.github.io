<!DOCTYPE html><html lang="en_us"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="author" content="Brody Liudzius"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/andrewh0/okcss@1/dist/ok.min.css"><script type="module" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async id="MathJax-script"></script></head><body><header><a href="/">Home</a><p><a href="/CECS_456">CECS 456</a></p></header><main><article><h1>Decision Tree Induction</h1><hr><table><tr><td style="min-width: 20vw"><h2>Contents</h2><hr><ul><li><a href="#Introduction">Introduction</a></li><li><a href="#Tree_Induction">Tree Induction</a><ul><li><a href="#Hunts_Algorithm">Hunt's Algorithm</a></li></ul></li><li><a href="#Splitting_Data">Splitting Data</a></li><li><a href="#Determining_Discretization_Points">Determining Discretization Points</a><ul><li><a href="#Purity">Purity</a></li><ul><li><a href="#Gini_Index">Gini Index</a></li><li><a href="#Entropy">Entropy</a></li><li><a href="#Classification_Error">Classification Error</a></li><li><a href="#Inpurity_of_a_Non-Leaf_Node">Impurity of a Non-Leaf Node</a></li></ul><li><a href="#Information_Gain">Information Gain</a></li><li><a href="#Gain_Ratio">Gain Ratio</a></li></ul></li><li><a href="#References">References</a></li></ul></td></tr></table><section><h2 id="Introduction">Introduction</h2><p><dfn>Decision Tree Induction</dfn> is a machine learning technique for classifying data.
A set of training data that has already been classified is used to train the model (induction), this training process will produce a trained model in the form of a completed decision tree.
The decision tree is a flowchart with each node representing the splitting of data by a certain attribute (decision trees that split data on multiple attributes per node are not discussed here).
Data will filter through the decision tree until it end up in a given leaf node.
The goal is that every leaf node only contains data that belongs to one class.
Data can then be classified by seeing which leaf it end up in.</p></section><section><h2 id="Tree_Induction">Tree Induction</h2><p>Induction is the process of developing the decision tree.
There are a number of  induction algorithm's such as Hunt's Algorithm, CART, ID3, C4.5, SLIQ, ans SPRINT.</p><section><h3 id="Hunts_Algorithm">Hunt's Algorithm</h3><p>Hunt's algorithm involves checking if data contains multiple classes.
If so, then split the data it into multiple groups (each group is a new node on the tree) using one of the data attributes.
Repeat this check on the nodes that were just created and split them if required.
Continue this process until all leaf nodes contain only a single class</p></section></section><section><h2 id="Splitting_Data">Splitting Data</h2><p>At each node on the decision tree, data will be split based on an attribute.
If the attribute is binary, then there will be two child nodes.
If the attribute is discrete, then the number of child nodes can be up to however many discrete values there are for that attribute.
If the attribute is continuous, then data must be <dfn>discretized</dfn>.
This means splitting the data into groups by, for example, sorting all data greater than a value into one child node and all data less than or equal into another child node.</p></section><section><h2 id="Determining_Discretization_Points">Determining Discretization Points</h2><p>For continuous attributes, a value needs to be chosen around which to discretize the data.
This can be done with the following steps:</p><ol><li>Sort the data by the attribute of interest </li><li>Calculate the <a href="#Purity">Impurity</a> of splitting the sorted data into groups at each point</li><li>Split the data at the point that had the lowest impurity. This point is most likely to occur on a class boundary in the training data (as shown below).</li></ol><figure><img src="/CECS456/DataSortedByAttribute.png" alt="A training dataset sorted by an attribute labeled 'Annual Income'. The data is binary classified as 'Yes' or 'No'. Boundaries between 'Yes' or 'No' groups in the sorted data are demarcated."><figcaption><i>Fig. 1</i> An example of some data sorted by the "Annual Income" attribute with class boundaries demarcated. Adapted from [1]</figcaption></figure><section><h3 id="Purity">Purity</h3><p><dfn>Purity</dfn> is a measure of how close a node is to containing only a single class.
Since the methods of calculation typically return 0 for pure data and a positive, non-zero value for impure data, they are referred to as measures of <em>Impurity</em>.
Three such methods are the Gini Index, Entropy, and Classification Error.</p><section><h4 id="Gini_Index">Gini Index</h4><p>The Gini index of a leaf node \(t\) is\[ \textrm{Impurity}(t)=1-\sum_{i=1}^{c-1} (P_i)^2 \tag{1} \]</p> where<ul><li>\(c\) is the number of classes in the dataset</li><li>\(P_i\) is the percentage of data at that node that belongs to class \(i\)</li></ul><p>and the maximum possible Gini index of a node is\[ \textrm{Impurity}_{max}(t)=1-\frac{1}{c} \tag{1.1} \]</p></section><section><h4 id="Entropy">Entropy</h4><p>The Entropy of a leaf node \(t\) is\[ \textrm{Impurity}(t)=-\sum_{i=1}^{c-1} P_i \cdot log_2(P_i) \tag{2} \]
and the maximum possible entropy of a node is
\[ \textrm{Impurity}_{max}(t)=log_2(c) \tag{2.1} \]</p></section><section><h4 id="Classification_Error">Classification Error</h4><p>The Classification Error of a leaf node \(t\) is\[ \textrm{Impurity}(t)=1-max(P_i) \tag{3} \]
where \(i\in \{1,...,c-1\}\)
and the maximum possible classification error of a node is
\[ \textrm{Impurity}_{max}(t)=1-\frac{1}{c} \tag{3.1} \]</p></section><section><h4 id="Inpurity_of_a_Non-Leaf_Node">Impurity of a Non-Leaf Node</h4><p>The impurity of a node that is not a leaf and thus has other decision nodes as its children is simply the weighted sum of the impurities of these nodes.
The sum is weigthed by the populations of these nodes.
For eaxmple, if there are two child nodes and one contains two-thirds of the data and the other contains one-third, then the summation is two-thirds multiplied by the impurity of the first node plus one-third multiplied by the impurity of the second node.
\[ \textrm{Impurity}_{\textrm{split}}(t)=\sum_{i=1}^{k} \frac{n_i}{n} \textrm{Impurity(t_i)} \tag{4} \]
where</p><ul><li>\(n\) is the number of data points belonging to this node (the sum of data points in all child nodes)</li><li>\(n_i\) is the number of data points belonging to the \(i^{th}\) child node</li><li>\(t_i\) is the \(i^{th}\) child node</li></ul></section></section><section><h3 id="Information_Gain">Information Gain</h3><p><dfn>Information gain</dfn> is a way to measure the increase in purity of a node before and after splitting the data within it.
As such, information gain will increase for more optimal splits.
It is useful in induction algorithms like C4.5.
\[ \textrm{Gain}_{\textrm{split}}(t)=\textrm{Impurity}(t)-\sum_{i=1}^{k}\textrm{Impurity}_{\textrm{split}}(t_i) \tag{5} \]</p></section><section><h3 id="Gain_Ratio">Gain Ratio</h3><p>The <dfn>Gain Ratio</dfn> is a method of adjusting the perceived gain of splitting a node into many partitions.
If a node a split in to many partitions that each contain only a small number of data points, this is less valuable even if the nodes are pure.
This is because it is more likely for a node with only a couple data points in it to be pure since there are less ways to split two data points into classes than say splitting ten data points into classes.
This can make it more ambiguous if the data was purified or if the training data just happens to <em>appear</em> pure under this scheme.
To avoid this issue, it is advantageous to penalize splits that produce many children with small populations.</p><p>To determine the gain ratio, the entropy (or information) of the partitioning system created by a given split, must be found:
\[ \textrm{Split Info}(t)=-\sum_{i=1}^{k}\frac{n_i}{n}log_2(\frac{n_i}{n}) \tag{6.1} \]
The gain ratio can then be defined as
\[ \textrm{Gain Ratio}=\frac{\textrm{Gain}_{\textrm{split}}}{\textrm{Split Info}} \tag{6} \]</p></section></section><hr><section><h2 id="References">References</h2><p>[1] P.-N. Tan, M. Steinbach, A. Karpatne, V. Kumar, <i>Introduction to Data Mining</i>,  2nd Edition. Pearson, 2018, ch. 3.</p><p>[2] A. Ghasemkhani, Class Lecture, CECS 456, College of Engineering, California State University, Long Beach, Spring 2024.</p></section></article></main><footer><center><small>Copyright Â© 2024 Brody Liudzius</small></center></footer></body></html>