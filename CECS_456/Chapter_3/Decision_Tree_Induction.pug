extends /default.pug

block title
    title Decision Tree Induction

append header
    p: a(href="/CECS_456") CECS 456

block main
    article
        h1 Decision Tree Induction
        hr

        +tableOfContents()
            li: a(href="#Introduction") Introduction
            li
                a(href="#Tree_Induction") Tree Induction
                ul: li: a(href="#Hunts_Algorithm") Hunt's Algorithm
            li: a(href="#Splitting_Data") Splitting Data
            li
                a(href="#Determining_Discretization_Points") Determining Discretization Points
                ul
                    li: a(href="#Purity") Purity
                    ul
                        li: a(href="#Gini_Index") Gini Index
                        li: a(href="#Entropy") Entropy
                        li: a(href="#Classification_Error") Classification Error
                        li: a(href="#Inpurity_of_a_Non-Leaf_Node") Impurity of a Non-Leaf Node
                    li: a(href="#Information_Gain") Information Gain
                    li: a(href="#Gain_Ratio") Gain Ratio
            li: a(href="#References") References

        section#Introduction
            p
                | #[dfn Decision Tree Induction] is a machine learning technique for classifying data.
                | A set of training data that has already been classified is used to train the model (induction), this training process will produce a trained model in the form of a completed decision tree.
                | The decision tree is a flowchart with each node representing the splitting of data by a certain attribute (decision trees that split data on multiple attributes per node are not discussed here).
                | Data will filter through the decision tree until it end up in a given leaf node.
                | The goal is that every leaf node only contains data that belongs to one class.
                | Data can then be classified by seeing which leaf it end up in.
        section#Tree_Induction
            h2 Tree Induction
            p
                | Induction is the process of developing the decision tree.
                | There are a number of  induction algorithm's such as Hunt's Algorithm, CART, ID3, C4.5, SLIQ, ans SPRINT.
            section#Hunts_Algorithm
                h3 Hunt's Algorithm
                p
                    | Hunt's algorithm involves checking if data contains multiple classes.
                    | If so, then split the data it into multiple groups (each group is a new node on the tree) using one of the data attributes.
                    | Repeat this check on the nodes that were just created and split them if required.
                    | Continue this process until all leaf nodes contain only a single class
        
        section#Splitting_Data
            h2 Splitting Data
            p
                | At each node on the decision tree, data will be split based on an attribute.
                | If the attribute is binary, then there will be two child nodes.
                | If the attribute is discrete, then the number of child nodes can be up to however many discrete values there are for that attribute.
                | If the attribute is continuous, then data must be #[dfn discretized].
                | This means splitting the data into groups by, for example, sorting all data greater than a value into one child node and all data less than or equal into another child node.
        
        section#Determining_Discretization_Points
            h2 Determining Discretization Points
            p
                | For continuous attributes, a value needs to be chosen around which to discretize the data.
                | This can be done with the following steps:
            ol
                li Sort the data by the attribute of interest 
                li Calculate the #[a(href="#Purity") Impurity] of splitting the sorted data into groups at each point
                li Split the data at the point that had the lowest impurity. This point is most likely to occur on a class boundary in the training data (as shown below).
            figure
                //- #toDo: Replace with an original (non-copyrighted) image
                img(src="/CECS456/DataSortedByAttribute.png" alt="A training dataset sorted by an attribute labeled 'Annual Income'. The data is binary classified as 'Yes' or 'No'. Boundaries between 'Yes' or 'No' groups in the sorted data are demarcated.")
                figcaption #[i Fig. 1] An example of some data sorted by the "Annual Income" attribute with class boundaries demarcated. Adapted from [1]
        
            section#Purity
                h3 Purity
                p
                    | #[dfn Purity] is a measure of how close a node is to containing only a single class.
                    | Since the methods of calculation typically return 0 for pure data and a positive, non-zero value for impure data, they are referred to as measures of #[em Impurity].
                    | Three such methods are the Gini Index, Entropy, and Classification Error.
                section#Gini_Index
                    h4 Gini Index
                    p The Gini index of a leaf node \(l\) is
                        | \[ \textrm{Impurity}(l)=1-\sum_{i=1}^{c-1} (P_i)^2 \tag{1} \]
                    |  where
                    ul
                        li \(c\) is the number of classes in the dataset
                        li \(P_i\) is the percentage of data at that node that belongs to class \(i\)
                    p and the maximum possible Gini index of a node is
                        | \[ \textrm{Impurity}_{max}(l)=1-\frac{1}{c} \tag{1.1} \]
                section#Entropy
                    h4 Entropy
                    p The Entropy of a leaf node \(l\) is
                        | \[ \textrm{Impurity}(l)=-\sum_{i=1}^{c-1} P_i \cdot log_2(P_i) \tag{2} \]
                        | and the maximum possible entropy of a node is
                        | \[ \textrm{Impurity}_{max}(l)=log_2(c) \tag{2.1} \]
                section#Classification_Error
                    h4 Classification Error
                    p The Classification Error of a leaf node \(l\) is
                        | \[ \textrm{Impurity}(l)=1-max(P_i) \tag{3} \]
                        | where \(i\in \{1,...,c-1\}\)
                        | and the maximum possible classification error of a node is
                        | \[ \textrm{Impurity}_{max}(l)=1-\frac{1}{c} \tag{3.1} \]
                section#Inpurity_of_a_Non-Leaf_Node
                    h4 Impurity of a Non-Leaf Node
                    p
                        | The impurity of a node that is not a leaf and thus has other decision nodes as its children is simply the weighted sum of the impurities of these nodes.
                        | The sum is weigthed by the populations of these nodes.
                        | For eaxmple, if there are two child nodes and one contains two-thirds of the data and the other contains one-third, then the summation is two-thirds multiplied by the impurity of the first node plus one-third multiplied by the impurity of the second node.
                    p
                        | If \(p\) is a parent node with \(k\) children, its impurity is
                        | \[ \textrm{Impurity}_{\textrm{split}}(p)=\sum_{i=1}^{k} \frac{n_i}{n} \textrm{Impurity}(q_i) \tag{4} \]
                        | where
                    ul
                        li \(n\) is the number of data points belonging to this node (the sum of data points in all child nodes)
                        li \(n_i\) is the number of data points belonging to the \(i^{th}\) child node
                        li \(q_i\) is the \(i^{th}\) child node
                    p If the children are not leaf nodes, then this formula becomes recursive.

            section#Information_Gain
                h3 Information Gain
                p
                    | #[dfn Information gain] is a way to measure the increase in purity of a node before and after splitting the data within it.
                    | As such, information gain will increase for more optimal splits.
                    | It is useful in induction algorithms like C4.5.
                    | \[ \textrm{Gain}_{\textrm{split}}(p)=\textrm{Impurity}(p)-\sum_{i=1}^{k}\textrm{Impurity}_{\textrm{split}}(q_i) \tag{5} \]
            
            section#Gain_Ratio
                h3 Gain Ratio
                p
                    | The #[dfn Gain Ratio] is a method of adjusting the perceived gain of splitting a node into many partitions.
                    | If a node a split in to many partitions that each contain only a small number of data points, this is less valuable even if the nodes are pure.
                    | This is because it is more likely for a node with only a couple data points in it to be pure since there are less ways to split two data points into classes than say splitting ten data points into classes.
                    | This can make it more ambiguous if the data was purified or if the training data just happens to #[em appear] pure under this scheme.
                    | To avoid this issue, it is advantageous to penalize splits that produce many children with small populations.
                p
                    | To determine the gain ratio, the entropy (or information) of the partitioning system created by a given split, must be found:
                    | \[ \textrm{Split Info}(p)=-\sum_{i=1}^{k}\frac{n_i}{n}log_2(\frac{n_i}{n}) \tag{6.1} \]
                    | The gain ratio can then be defined as
                    | \[ \textrm{Gain Ratio}(p)=\frac{\textrm{Gain}_{\textrm{split}}(p)}{\textrm{Split Info}(p)} \tag{6} \]

        
        hr

        section
            h2#References References
            p [1] P.-N. Tan, M. Steinbach, A. Karpatne, V. Kumar, #[i Introduction to Data Mining],  2nd Edition. Pearson, 2018, ch. 3.
            p [2] A. Ghasemkhani, Class Lecture, CECS 456, College of Engineering, California State University, Long Beach, Spring 2024.